"""
Discrete Variational Autoencoder Class

Author: Eric Drechsler (eric_drechsler@sfu.ca)
"""

import torch
from torch import nn
from models.autoencoderbase import AutoEncoderBase

from utils.networks import HierarchicalEncoder, BasicDecoder
from models.rbm import RBM
from utils.distributions import Bernoulli

#logging module with handmade settings. (for testing)
from DiVAE import logging
logger = logging.getLogger(__name__)

class DiVAE(AutoEncoderBase):
    def __init__(self, **kwargs):
        super(DiVAE, self).__init__(**kwargs)
        self._model_type="DiVAE"
        
        #TODO can this be done through inheritance from AutoEncoder?
        self._decoder_nodes=[]
        
        #TODO hydra: is there a built-in feature for list comprehension?
        dec_hidden_node_list=[int(i) for i in self._config.model.decoder_hidden_nodes.split(",")]
        dec_node_list=[(int(self._latent_dimensions*self._config.model.n_latent_hierarchy_lvls))]+dec_hidden_node_list+[self._flat_input_size]

        for num_nodes in range(0,len(dec_node_list)-1):
            nodepair=(dec_node_list[num_nodes],dec_node_list[num_nodes+1])
            self._decoder_nodes.append(nodepair)

        #TODO change names globally
        #TODO one wd factor for both SimpleDecoder and encoder
        self.weight_decay_factor=self._config.engine.weight_decay_factor
        
        # TODO - Model attributes can be directly imported from the config model dict
        # by iterating over and using (k,v) in dict.items() and using setattr(self, k, v)
        
        #ENCODER SPECIFICS
        #number of hierarchy levels in encoder. At each hierarchy level an latent layer is formed.
        self.n_latent_hierarchy_lvls=self._config.model.n_latent_hierarchy_lvls

        #number of latent nodes in the prior - output nodes for each level of
        #the hierarchy.
        self.n_latent_nodes=self._config.model.n_latent_nodes
        
        # number of layers in encoder before latent layer. These layers map
        #input to the latent layer. 
        self.n_encoder_layers=self._config.model.n_encoder_layers

        #each hierarchy has NN with n_encoder_layers_enc layers
        #number of deterministic nodes in each encoding layer. 
        self.n_encoder_layer_nodes=self._config.model.n_encoder_layer_nodes

        #added to output activation of last decoder layer in forward call
        self._train_bias=self.set_train_bias()

    def set_train_bias(self):
        """
        this treatment is a recommendation from the Hinton paper (A Practical Guide to Training Restricted Boltzmann Machines) on how to
        initialise biases. 
        It is usually helpful to initialize the bias of visible unit i to
        log[pi/(1âˆ’pi)] 
        where pi is the proportion of training vectors in which unit i is on. If this is not done, 
        the early stage of learning will use the hidden units to make i turn on with a probability of approximately pi.

        Returns:
            
        """
        clipped_mean=torch.clamp(self._dataset_mean,0.001,0.999).detach()
        return -torch.log(1/clipped_mean-1)

    def create_networks(self):
        logger.debug("Creating Network Structures")
        self.encoder=self._create_encoder()
        self.prior=self._create_prior()
        self.decoder=self._create_decoder()
        return
    
    def _create_encoder(self):
        logger.debug("ERROR _create_encoder dummy implementation")
        return HierarchicalEncoder(
            input_dimension=self._flat_input_size,
            n_latent_hierarchy_lvls=self.n_latent_hierarchy_lvls,
            n_latent_nodes=self.n_latent_nodes,
            n_encoder_layer_nodes=self.n_encoder_layer_nodes,
            n_encoder_layers=self.n_encoder_layers,
            skip_latent_layer=False,
            cfg=self._config)

    def _create_decoder(self):
        logger.debug("_create_decoder")
        #Identity output_activation_fct, as this sigmoid is called manually in forward()
        return BasicDecoder(node_sequence=self._decoder_nodes, activation_fct=self._activation_fct, output_activation_fct=nn.Identity(), cfg=self._config)

    def _create_prior(self):
        logger.debug("_create_prior")
        num_rbm_nodes_per_layer=self._config.model.n_latent_hierarchy_lvls*self._latent_dimensions//2
        return RBM(n_visible=num_rbm_nodes_per_layer,n_hidden=num_rbm_nodes_per_layer)
   
    def weight_decay_loss(self):
        #TODO Implement weight decay
        logger.debug("ERROR weight_decay_loss NOT IMPLEMENTED")
        return NotImplementedError

    def loss(self, input_data, fwd_out):
        logger.debug("loss")

        #1) Gradients of KL Divergence     
        kl_loss_per_sample=self.kl_divergence(fwd_out.posterior_distributions,fwd_out.posterior_samples)
        
        # Bug 1 - KL loss per sample returns an int of 0
        try:
            kl_loss = torch.mean(kl_loss_per_sample)
        except:
            kl_loss = torch.mean(torch.tensor(float(kl_loss_per_sample)))
            
        #2) AE loss
        ae_loss_matrix=-fwd_out.output_distribution.log_prob_per_var(input_data.view(-1, self._flat_input_size))
        #loss is the sum of all variables (pixels) per sample (event in batch)
        ae_loss_per_sample = torch.sum(ae_loss_matrix,1)
        ae_loss = torch.mean(ae_loss_per_sample) 
        
        neg_elbo = ae_loss + kl_loss
        
        #3) weight decay loss
        #TODO add this for encoder, decoder, prior
        #TODO include the weight decay regularisation in the loss to penalise
        #complexity
        loss=neg_elbo#+weight_decay_loss  
        return {"loss":loss, "ae_loss":ae_loss, "kl_loss":kl_loss}
    
    def kl_divergence(self, posterior_distribution , posterior_samples):
        logger.debug("kl_divergence")
        #posterior_distribution: distribution with logits from each hierarchy level/layer
        #posterior_samples: reparameterised output of posterior_distribution
        if len(posterior_distribution)>1 and self.training:

            logit_list=[]
            samples_marginalised=[]
            for lvl in range(len(posterior_distribution)):

                current_post_dist=posterior_distribution[lvl]
                current_post_samples=posterior_samples[lvl]

                logits=torch.clamp(current_post_dist.logits,min=-88,max=88)
                logit_list.append(logits)
        
                #TODO this step is not clear anymore: where was this motivated
                #in the paper? 
                if lvl==len(posterior_distribution)-1:
                    samples_marginalised.append(torch.sigmoid(logits))
                else:
                    zero_mask=torch.zeros(current_post_samples.size())
                    one_mask=torch.ones(current_post_samples.size())
                    post_sample_marginalised=torch.where(current_post_samples>0.0,one_mask,zero_mask)
                    samples_marginalised.append(post_sample_marginalised)

            logits_concat=torch.cat(logit_list,1)
            samples_marginalised_concat=torch.cat(samples_marginalised,1)

            kl_div_posterior_distribution=self.kl_div_posterior_gradient(
                posterior_logits=logits_concat,
                posterior_binary_samples=samples_marginalised_concat)
                
            kl_div_prior=self.kl_div_prior_gradient(
                posterior_logits=logits_concat,
                posterior_binary_samples=samples_marginalised_concat)  #DVAE Eq11 - gradient of prior   
            kld=kl_div_prior+kl_div_posterior_distribution 
            return kld
        else: # either this posterior only has one latent layer or we are not looking at training
            # #this posterior is not hierarchical - a closed analytical form for the KLD term can be constructed
            # #the mean-field solution (n_latent_hierarchy_lvls == 1) reduces to log_ratio = 0.
            # logger.debug("kld for evaluation/training of one layer posterior")
            return 0

    def kl_div_prior_gradient(self, posterior_logits, posterior_binary_samples):
        logger.debug("kl_div_prior_gradient")

        #see quadrant implementation
        #DVAE Eq11 - gradient of prior
        #gradient of the KLD between posterior and prior wrt to prior
        #parameters theta, i.e. generative model parameters.

        # Ep(z,theta) = -zT*Weights*z - zT*bias

        ######
        # POSITIVE: samples z_i from posterior
        ####
        #logits to probabilities
        posterior_probs=torch.sigmoid(posterior_logits)
        positive_probs=posterior_probs.detach()
        
        #samples from posterior are labelled positive
        positive_samples=posterior_binary_samples.detach()

        n_split=positive_samples.size()[1]//2
        positive_samples_left,positive_samples_right=torch.split(positive_samples,split_size_or_sections=int(n_split),dim=1)
        
        #-zT*Weights*z 
        pos_first_term=torch.matmul(positive_samples_left,self.prior.get_weights())*positive_samples_right
       
        rbm_bias_left=self.prior.get_visible_bias()
        rbm_bias_right=self.prior.get_hidden_bias()
        rbm_bias=torch.cat([rbm_bias_left,rbm_bias_right])
        
        #zT*bias
        #TODO is it correct to use probabilities here? shouldn't these be the
        #binary samples z?
        pos_sec_term=positive_probs*rbm_bias
        
        #positive d/dtheta Ep(z,theta) (z from posterior q, first term in Eq11)
        pos_kld_per_sample=-(torch.sum(pos_first_term,axis=1)+torch.sum(pos_sec_term,axis=1))
        
        ######
        # NEGATIVE: samples z_i from prior (RBM)
        ####

        #TODO what's the impact of doing gibbs sampling here? Is this the
        #best possible sampling procedure?
        rbm_samples=self.prior.get_samples_kld(approx_post_samples=positive_samples_left,n_gibbs_sampling_steps=1)
        negative_samples=rbm_samples.detach()
        
        # Ep(z,theta) = -zT*Weights*z - zT*bias
        n_split=negative_samples.size()[1]//2
        negative_samples_left,negative_samples_right=torch.split(negative_samples,split_size_or_sections=int(n_split),dim=1)
        neg_first_term=torch.matmul(negative_samples_left,self.prior.get_weights())*negative_samples_right
        
        #TODO in the positive case we use probabilities, in the negative case
        #here samples from the prior... is this correct? Relates to the TODO above.
        neg_sec_term=negative_samples*rbm_bias
        neg_kld_per_sample=(torch.sum(neg_first_term,axis=1)+torch.sum(neg_sec_term,axis=1))
        
        kld_per_sample=pos_kld_per_sample+neg_kld_per_sample
        return kld_per_sample

    def kl_div_posterior_gradient(self, posterior_logits, posterior_binary_samples):
        #see quadrant implementation
        #DVAE Eq12
        #gradient of the KLD between posterior and prior wrt to posterior
        #parameters phi.
        logger.debug("kl_div_posterior_gradient")

        #logits to probabilities. dq/dphi in Eq12. The only differentiated component
        #in this calculation!
        posterior_upper_bound = 0.999*torch.ones_like(posterior_logits)
        posterior_probs=torch.min(posterior_upper_bound, torch.sigmoid(posterior_logits))
        
        #binarised/discretised samples from posterior to RBM layers
        n_split=int(posterior_binary_samples.size()[1]//2)
        rbm_samples_left,rbm_samples_right=torch.split(posterior_binary_samples,split_size_or_sections=n_split,dim=1)

        #the following prepares the variables in the calculation into a specific
        #format, so it's easier to read later.
        rbm_bias_left=self.prior.get_visible_bias()
        rbm_bias_right=self.prior.get_hidden_bias()

        rbm_bias=torch.cat([rbm_bias_left,rbm_bias_right])
        rbm_weight=self.prior.get_weights()

        # this is transposed, so we multiply what we call "right hand" ("hidden layer")
        # samples with right rbm nodes
        rbm_activation_right=torch.matmul(rbm_samples_right,rbm_weight.t())
        rbm_activation_left=torch.matmul(rbm_samples_left,rbm_weight)

        #corresponds to zT*W
        rbm_activation=torch.cat([rbm_activation_right,rbm_activation_left],1)
        
        #Eq 12, (1-z)/(1-q). 
        hadamard_scaling= (1.0 - posterior_binary_samples) / (1.0 - posterior_probs)
        hadamard_scaling_left,hadamard_scaling_right=torch.split(hadamard_scaling, split_size_or_sections=int(n_split),dim=1)
        
        #TODO check again if this is actually true
        #In order to be able to put the differentiated component dq/dphi outside
        #the bracketes, the above is transformed:
        hadamard_scaling_with_ones=torch.cat([hadamard_scaling_left,torch.ones(hadamard_scaling_right.size())],axis=1)
        
        #this gives all terms of Eq12 except dq/dphi
        with torch.no_grad():
            undifferentiated_component=posterior_logits-rbm_bias-rbm_activation*hadamard_scaling_with_ones
            undifferentiated_component=undifferentiated_component.detach()
        
        #final KLD gradient wrt phi
        kld_per_sample = torch.sum(undifferentiated_component * posterior_probs, dim=1)

        return kld_per_sample

    #TODO experimental for now. The sampling technique in the prior is not
    #cross checked with anything.
    def generate_samples(self):
        prior_samples=[]
        #how many samples (i.e. digits) to look at
        for i in range(0,self._config.n_generate_samples):
            prior_sample = self.prior.get_samples(
                n_latent_nodes=self.n_latent_nodes,
                n_gibbs_sampling_steps=self._config.engine.n_gibbs_sampling_steps, 
                sampling_mode=self._config.engine.sampling_mode)
            prior_sample = torch.cat(prior_sample)
            prior_samples.append(prior_sample)
        
        prior_samples=torch.stack(prior_samples)
        output_activations = self.decoder.decode(prior_samples)
        output_activations = output_activations+self._train_bias
        output_distribution = Bernoulli(logit=output_activations)
        output=torch.sigmoid(output_distribution.logits)
        output.detach()

        return output

    def forward(self, input_data):
        logger.debug("forward")

        #see definition for explanation
        out=self._output_container.clear()

        #TODO data prep - study if this does good things
        input_data_centered=input_data.view(-1, self._flat_input_size)-self._dataset_mean
        #Step 1: Feed data through encoder 
        out.posterior_distributions, out.posterior_samples = self.encoder.hierarchical_posterior(input_data_centered)
        posterior_samples_concat=torch.cat(out.posterior_samples,1)
        #Step 2: take samples zeta and reconstruct output with decoder
        output_activations = self.decoder.decode(posterior_samples_concat)

        out.output_activations = output_activations+self._train_bias
        out.output_distribution = Bernoulli(logit=out.output_activations)
        out.output_data = torch.sigmoid(out.output_distribution.logits)
        
        return out

if __name__=="__main__":
    logger.debug("Testing Model Setup") 
    model=DiVAE()
    print(model)
    logger.debug("Success")
    pass