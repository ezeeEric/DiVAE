[model_specifications]
model_type=DiVAE
data_type=mnist
n_train_samples = 1200
n_test_samples = 100
n_batch_samples = 400
n_epochs = 100
learning_rate = 0.001
activation_fct=tanh
n_latent_hierarchy_lvls=4
n_latent_nodes=100
n_encoder_layer_nodes=200
n_encoder_layers=2
decoder_hidden_nodes=200,200
weight_decay_factor=1e-4
#generation
n_plot_samples=5
n_generate_samples=10
n_gibbs_sampling_steps=100
sampling_mode=gibbs_flat
[all]
tag=default_210201
output_path=./output/divae_mnist/
beta_smoothing_fct=4